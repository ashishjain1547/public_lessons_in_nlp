<head>
    <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    
    <!-- Google AdSense Using Machine Learning Code -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3071098372371409",
            enable_page_level_ads: true
        });
    </script>

    <style>
        .customTempCodeHolderForSocialMedia {
            display: none;
        }

        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }

        i.ibb {
            display: block;
            background-color: black;
            color: white;
        }

        i.ibg {
            display: block;
            background-color: rgb(50, 50, 50);
            color: white;
        }


        ul.no-bullets {
            list-style-type: none;
            /* Remove bullets */
            padding: 0;
            /* Remove padding */
            margin: 0;
            /* Remove margins */
        }

        li.customLi {
            float: left;
        }

        li.customLi>span.customSpan {
            display: block;
            padding: 8px;
            background-color: #dddddd;
        }
    </style>
</head>
<!-- End of 'Personal Posts Menu Template For Copy-Paste'. It started from the top of the page from <HEAD> tag. -->

<pre>
Infosys Certified Natural Langugage Processing Professional:
The test comprises of 21 questions.

Five questions asked in the test are:

1. What is IDF?

2. What are Alpha and Beta in Latent Dirichlet Allocation?
Among alpha and beta, which one relates to 'number of topics'?

3. Which vectorizing technique gives more weightage to 'stop' words:
3.a. GloVe
3.b. Word2Vec
3.c. TF-IDF 
3.d. Bag of words 

4. What is the purpose of Linear Discriminant Analysis?

5. Which of the following is a tokenization step?
5.a. getting unique words from text.
5.b. getting most undivisable unit from text

6. Does the word order matter in LDA (Latent Dirichlet Allocation)?

7. Which of the given word pairs shows stemming?
</pre>

<span style="opacity: 0;">Tags: Technology,Natural Language Processing,</span>